1) I would expect the throughput to double due to parallelism of tasks in my dataflow graph. Latency would decrease
but not by double. So maybe latency wuld decrease around 1.3 or 1.5 times since not all tasks exhibit parallelism

2) For P = 1, the expected throughput was about 15 items/second. Which means with scaling, 100 items shoudl take 15 seconds, 
1000 should take 150 seconds, 10000 should take 1500 seconds, 100000 should take 15000 seconds and 1000000 shoudl take 
150000 seconds. However, looking at the observed throughput, the graph indicates that although input sizes of 1-1000 were close
to the expected throughput, when scaling to inputs sizes of 10,000 or 1,000,000 the expected throughput was much larger than the 
observed throughput. For example for 1,000,000 input size at Parallism 1, it processed only around 27,000 instead of 150,000. There
is a similar trend with the rest of the Parallism values, althought at P = 2 and P = 4, when scaling to higher input sizes, the 
observed throughput is higher to around 30,000 items/ second, but still not even close to the expected. For latency, the 
expected latency should be constant, around 10 seconds for each input size since data parallelism doesn't effect latency a whole lot.
Based on the observed this is mostly true, but again with input sizes of 10,000 or 1,000,000 latancy becomes around 70 seconds. The
same reains true for higher P values, except P = 2 which acutally only has a latency of around a minute. This is most likely
due to spark making partitions and keeping track of them.

3) I conjecture that the theoretical model does not accurately represent the actual runtime of the application at different input
sizes. Mainly, the theoretical model suggests throuput being doubled linarly for different input sizes. However, this does not 
hold true for our observed latency when testing inputs higher that 10,000 items. Similarly, latency should remain mostly constant,
but it reaches a large spike it time when having input sizes over 1,000,000 items. This is mostly due to real world issues with
spark as well as system restrictions. For example, for larger P values, specifically 16. The throughput and latency are noticably
worse than from smaller P values of 2 and 4. This feels strictly due to spark and how creating too many partitions and keeping track
of what is in them on one machine can cause more harm than good. In terms of system restrictions, when running part 8_b specifically,
running that part with more that 10,000 inputs ends up using 100% of the CPU power in the codespace and therefore explains why
throughput and latency are off from the expected outcomes.